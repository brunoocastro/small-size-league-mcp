{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import tiktoken\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import SKLearnVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_sitemap(url_sitemap):\n",
    "    \"\"\"\n",
    "    Extract URLs and metadata from a sitemap XML file.\n",
    "    PS: Designed for RoboCup Small Size League sitemap\n",
    "    \n",
    "    Args:\n",
    "        url_sitemap (str): URL of the sitemap to parse\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing URL information\n",
    "        Each dictionary contains:\n",
    "            - url: The page URL\n",
    "            - lastmod: Last modification date\n",
    "            - changefreq: How often the page changes\n",
    "            - priority: Page priority (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch sitemap\n",
    "        response = requests.get(url_sitemap)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        # Parse XML with lxml parser\n",
    "        soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "        \n",
    "        # Initialize results list\n",
    "        urls_info = []\n",
    "        \n",
    "        # Check if this is a sitemap index\n",
    "        if soup.find('sitemapindex'):\n",
    "            logger.info(\"Found sitemap index, processing sub-sitemaps...\")\n",
    "            # Get all sitemap URLs\n",
    "            sitemap_urls = [sitemap.find('loc').text for sitemap in soup.find_all('sitemap')]\n",
    "            \n",
    "            # Process each sub-sitemap\n",
    "            for sitemap_url in sitemap_urls:\n",
    "                try:\n",
    "                    sub_response = requests.get(sitemap_url)\n",
    "                    sub_response.raise_for_status()\n",
    "                    sub_soup = BeautifulSoup(sub_response.content, 'lxml-xml')\n",
    "                    \n",
    "                    # Process URLs in sub-sitemap\n",
    "                    for url_elem in sub_soup.find_all('url'):\n",
    "                        url_info = extract_url_info(url_elem)\n",
    "                        if url_info:\n",
    "                            urls_info.append(url_info)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing sub-sitemap {sitemap_url}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        else:\n",
    "            # Process URLs in single sitemap\n",
    "            logger.info(\"Processing single sitemap...\")\n",
    "            for url_elem in soup.find_all('url'):\n",
    "                url_info = extract_url_info(url_elem)\n",
    "                if url_info:\n",
    "                    urls_info.append(url_info)\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(urls_info)} URLs from sitemap\")\n",
    "        return urls_info\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching sitemap: {str(e)}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing sitemap: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_url_info(url_elem):\n",
    "    \"\"\"\n",
    "    Extract information from a URL element in the sitemap.\n",
    "    \n",
    "    Args:\n",
    "        url_elem: BeautifulSoup element containing URL information\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing URL information or None if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract required URL\n",
    "        url = url_elem.find('loc')\n",
    "        if not url:\n",
    "            return None\n",
    "            \n",
    "        # Extract optional fields with defaults\n",
    "        lastmod = url_elem.find('lastmod')\n",
    "        changefreq = url_elem.find('changefreq')\n",
    "        priority = url_elem.find('priority')\n",
    "        \n",
    "        # Build info dictionary\n",
    "        url_info = {\n",
    "            'url': url.text.strip(),\n",
    "            'lastmod': lastmod.text.strip() if lastmod else None,\n",
    "            'changefreq': changefreq.text.strip() if changefreq else None,\n",
    "            'priority': float(priority.text.strip()) if priority else 0.5\n",
    "        }\n",
    "        \n",
    "        return url_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting URL info: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing single sitemap...\n",
      "INFO:__main__:Successfully extracted 115 URLs from sitemap\n"
     ]
    }
   ],
   "source": [
    "urls_info = extract_urls_from_sitemap(\"https://ssl.robocup.org/page-sitemap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_urls = [\n",
    "  \"https://robocup-ssl.github.io/ssl-goals/sslgoals.html\"\n",
    "  \"https://robocup-ssl.github.io/ssl-rules/sslrules.html\"\n",
    "  \"https://github.com/orgs/RoboCup-SSL/repositories\"\n",
    "  \"https://ssl.robocup.org/rules/\",\n",
    "  \"https://ssl.robocup.org/tournament-rules/\",\n",
    "  \"https://ssl.robocup.org/technical-overview-of-the-small-size-league/\",\n",
    "  \"https://ssl.robocup.org/tournament-organization/\",\n",
    "  \"https://ssl.robocup.org/divisions/\",\n",
    "  \"https://ssl.robocup.org/open-source-contributions/\",\n",
    "  \"https://ssl.robocup.org/history-of-open-source-submissions/\",\n",
    "  \"https://ssl.robocup.org/scientific-publications/\",\n",
    "  \"https://ssl.robocup.org/team-description-papers/\",\n",
    "  \"https://ssl.robocup.org/robocups/robocup-2025/robocup-2025-teams/\"\n",
    "  \"https://ssl.robocup.org/robocups/\",\n",
    "  \"https://ssl.robocup.org/history-of-technical-challenges/\",\n",
    "  \"https://ssl.robocup.org/match-statistics/\",\n",
    "  \"https://ssl.robocup.org/contact/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs: 117\n",
      "First 2 URLs: ['https://ssl.robocup.org/game-logs/', 'https://ssl.robocup.org/robocups/robocup-2022/robocup-2022-qualification/']\n",
      "Last 2 URLs: ['https://ssl.robocup.org/robocups/robocup-2019/robocup-2019-open-source-submissions/', 'https://ssl.robocup.org/contact/']\n"
     ]
    }
   ],
   "source": [
    "extracted_urls = [info['url'] for info in urls_info]\n",
    "\n",
    "website_urls = default_urls + extracted_urls\n",
    "\n",
    "website_urls = list(set(website_urls))\n",
    "\n",
    "\n",
    "print(f\"Total URLs: {len(website_urls)}\")\n",
    "print(f\"First 2 URLs: {website_urls[:2]}\")\n",
    "print(f\"Last 2 URLs: {website_urls[-2:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Target the main article content for documentation \n",
    "    main_content = soup.find(\"article\", class_=\"status-publish\")\n",
    "    \n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_site():\n",
    "    \"\"\"\n",
    "    Load information from the official website.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading website...\")\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # Show the progress\n",
    "    for url in tqdm(website_urls, desc=\"Processing URLs and loading page content\", total=len(website_urls), unit=\"URLs\"):\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url,\n",
    "            max_depth=5,\n",
    "            extractor=bs4_extractor,\n",
    "        )\n",
    "\n",
    "        # Load documents using lazy loading (memory efficient)\n",
    "        docs_lazy = loader.lazy_load()\n",
    "\n",
    "        # Load documents and track URLs\n",
    "        for d in docs_lazy:\n",
    "            docs.append(d)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from website.\")\n",
    "    \n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in tqdm(docs, desc=\"Counting tokens in documents\", total=len(docs), unit=\"documents\"):\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "        \n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "    \n",
    "    return docs, tokens_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_full_website(documents):\n",
    "    \"\"\" Save the documents to a file \"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    output_filename = \"full_website.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get('source', 'Unknown URL')\n",
    "            \n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i+1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "    \n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size=8,000 creates relatively large chunks for comprehensive context\n",
    "    # chunk_overlap=500 ensures continuity between chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=8000,  \n",
    "        chunk_overlap=500  \n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "    \n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "    \n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "    \n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "    \n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "        \n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "    \n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    \n",
    "    # Create vector store from documents using SKLearn\n",
    "    persist_path = os.getcwd()+\"/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_path=persist_path,\n",
    "        serializer=\"parquet\",\n",
    "        \n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "    \n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the site scrapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading website...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs and loading page content: 100%|██████████| 117/117 [04:31<00:00,  2.32s/URLs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 224 documents from website.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting tokens in documents: 100%|██████████| 224/224 [00:00<00:00, 618.42documents/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in loaded documents: 174455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_site()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the amount of documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the amount of tokens report\n",
    "len(tokens_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document has 434 tokens. Content:\n",
      "page_content='The league attempts to create log files of all official matches during each RoboCup. They contain the timestamped ProtoBuf messages of ssl-vision, ssl-game-controller and vision-tracker producers like the autoRefs.\n",
      "Software\n",
      "The league provides multiple tools to deal with log files of SSL games:\n",
      "\n",
      "ssl-logtools (C++): Original tooling with recorder and player\n",
      "ssl-go-tools (Go): Set of CLI tools to record, play and analyze log files\n",
      "\n",
      "The ssl-logtools can be considered legacy and are not maintained anymore. However, the ssl-go-tools do not provide a UI for the player yet.\n",
      "File Format\n",
      "Each log file starts with the following header:\n",
      "\n",
      "1: String – File type (“SSL_LOG_FILE”) 2: Int32 – Log file format version \n",
      "\n",
      "Format version 1 encodes the protobuf messages in the following format:\n",
      "\n",
      "1: Int64 – Receiver timestamp in ns 2: Int32 – Message type 3: Int32 – Size of binary protobuf message 4: String – Binary protobuf message \n",
      "\n",
      "The message types are:\n",
      "\n",
      "MESSAGE_BLANK = 0 (ignore message)MESSAGE_UNKNOWN = 1 (try to guess message type by parsing the data)MESSAGE_SSL_VISION_2010 = 2MESSAGE_SSL_REFBOX_2013 = 3MESSAGE_SSL_VISION_2014 = 4MESSAGE_SSL_VISION_TRACKER_2020 = 5MESSAGE_SSL_INDEX_2021 = 6\n",
      "\n",
      "Files can optionally contain an index at the end of the file for random access to messages (for fast seeking for example). An indexed log file has a message of type MESSAGE_SSL_INDEX_2021 as the last message. The value of the timestamp is irrelevant. The message is not in protobuf format, but in simple binary format for easy relative access from the end of the file:\n",
      "\n",
      "1: []Int64 – Array with byte-aligned offsets, starting at the beginning of the file2: Int64 – Offset from the end of the file to the beginning of the index message3: String – Index marker (“INDEXED”) to quickly check if a file is indexed\n",
      "\n",
      "Storage locations\n",
      "See Collected Data' metadata={'source': 'https://ssl.robocup.org/game-logs/', 'content_type': 'text/html; charset=UTF-8', 'title': 'Game Logs – Small Size League | RoboCup Soccer', 'language': 'en-US'}\n"
     ]
    }
   ],
   "source": [
    "# Seeing a preview of a document with the number of tokens\n",
    "print(f\"First document has {tokens_per_doc[0]} tokens. Content:\\n{documents[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents concatenated into full_website.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the documents to a file\n",
    "save_full_website(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n",
      "Created 224 chunks from documents.\n",
      "Total tokens in split documents: 174455\n"
     ]
    }
   ],
   "source": [
    "# Split the documents\n",
    "split_docs = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SKLearnVectorStore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (913,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create the vector store\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vectorstore = \u001b[43mcreate_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mcreate_vectorstore\u001b[39m\u001b[34m(splits)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Create vector store from documents using SKLearn\u001b[39;00m\n\u001b[32m     21\u001b[39m persist_path = os.getcwd()+\u001b[33m\"\u001b[39m\u001b[33m/sklearn_vectorstore.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m vectorstore = \u001b[43mSKLearnVectorStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSKLearnVectorStore created successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m vectorstore.persist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/www/personal/small-size-league-mcp/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:844\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    842\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/www/personal/small-size-league-mcp/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/sklearn.py:353\u001b[39m, in \u001b[36mSKLearnVectorStore.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, persist_path, **kwargs)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m    344\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     **kwargs: Any,\n\u001b[32m    351\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mSKLearnVectorStore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    352\u001b[39m     vs = SKLearnVectorStore(embedding, persist_path=persist_path, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     \u001b[43mvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/www/personal/small-size-league-mcp/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/sklearn.py:209\u001b[39m, in \u001b[36mSKLearnVectorStore.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28mself\u001b[39m._metadatas.extend(metadatas \u001b[38;5;129;01mor\u001b[39;00m ([{}] * \u001b[38;5;28mlen\u001b[39m(_texts)))\n\u001b[32m    208\u001b[39m \u001b[38;5;28mself\u001b[39m._ids.extend(_ids)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_update_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/www/personal/small-size-league-mcp/.venv/lib/python3.12/site-packages/langchain_community/vectorstores/sklearn.py:217\u001b[39m, in \u001b[36mSKLearnVectorStore._update_neighbors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._embeddings) == \u001b[32m0\u001b[39m:\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SKLearnVectorStoreException(\n\u001b[32m    215\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo data was added to SKLearnVectorStore.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    216\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m \u001b[38;5;28mself\u001b[39m._embeddings_np = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_np\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28mself\u001b[39m._neighbors.fit(\u001b[38;5;28mself\u001b[39m._embeddings_np)\n\u001b[32m    219\u001b[39m \u001b[38;5;28mself\u001b[39m._neighbors_fitted = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (913,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done.\n",
    "\n",
    "With the vector store created, we can now create a retriever to get relevant documents.\n",
    "\n",
    "Let's do some tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever to get relevant documents (k=3 means return top 3 matches)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant documents for the query\n",
    "query = \"How to submit a paper?\"    \n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata['source'])\n",
    "    print(d.page_content[0:500])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
