{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import tiktoken\n",
    "import logging\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import SKLearnVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls_from_sitemap(url_sitemap):\n",
    "    \"\"\"\n",
    "    Extract URLs and metadata from a sitemap XML file.\n",
    "    PS: Designed for RoboCup Small Size League sitemap\n",
    "    \n",
    "    Args:\n",
    "        url_sitemap (str): URL of the sitemap to parse\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing URL information\n",
    "        Each dictionary contains:\n",
    "            - url: The page URL\n",
    "            - lastmod: Last modification date\n",
    "            - changefreq: How often the page changes\n",
    "            - priority: Page priority (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch sitemap\n",
    "        response = requests.get(url_sitemap)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        \n",
    "        # Parse XML with lxml parser\n",
    "        soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "        \n",
    "        # Initialize results list\n",
    "        urls_info = []\n",
    "        \n",
    "        # Check if this is a sitemap index\n",
    "        if soup.find('sitemapindex'):\n",
    "            logger.info(\"Found sitemap index, processing sub-sitemaps...\")\n",
    "            # Get all sitemap URLs\n",
    "            sitemap_urls = [sitemap.find('loc').text for sitemap in soup.find_all('sitemap')]\n",
    "            \n",
    "            # Process each sub-sitemap\n",
    "            for sitemap_url in sitemap_urls:\n",
    "                try:\n",
    "                    sub_response = requests.get(sitemap_url)\n",
    "                    sub_response.raise_for_status()\n",
    "                    sub_soup = BeautifulSoup(sub_response.content, 'lxml-xml')\n",
    "                    \n",
    "                    # Process URLs in sub-sitemap\n",
    "                    for url_elem in sub_soup.find_all('url'):\n",
    "                        url_info = extract_url_info(url_elem)\n",
    "                        if url_info:\n",
    "                            urls_info.append(url_info)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing sub-sitemap {sitemap_url}: {str(e)}\")\n",
    "                    continue\n",
    "                    \n",
    "        else:\n",
    "            # Process URLs in single sitemap\n",
    "            logger.info(\"Processing single sitemap...\")\n",
    "            for url_elem in soup.find_all('url'):\n",
    "                url_info = extract_url_info(url_elem)\n",
    "                if url_info:\n",
    "                    urls_info.append(url_info)\n",
    "        \n",
    "        logger.info(f\"Successfully extracted {len(urls_info)} URLs from sitemap\")\n",
    "        return urls_info\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Error fetching sitemap: {str(e)}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing sitemap: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def extract_url_info(url_elem):\n",
    "    \"\"\"\n",
    "    Extract information from a URL element in the sitemap.\n",
    "    \n",
    "    Args:\n",
    "        url_elem: BeautifulSoup element containing URL information\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing URL information or None if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract required URL\n",
    "        url = url_elem.find('loc')\n",
    "        if not url:\n",
    "            return None\n",
    "            \n",
    "        # Extract optional fields with defaults\n",
    "        lastmod = url_elem.find('lastmod')\n",
    "        changefreq = url_elem.find('changefreq')\n",
    "        priority = url_elem.find('priority')\n",
    "        \n",
    "        # Build info dictionary\n",
    "        url_info = {\n",
    "            'url': url.text.strip(),\n",
    "            'lastmod': lastmod.text.strip() if lastmod else None,\n",
    "            'changefreq': changefreq.text.strip() if changefreq else None,\n",
    "            'priority': float(priority.text.strip()) if priority else 0.5\n",
    "        }\n",
    "        \n",
    "        return url_info\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting URL info: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing single sitemap...\n",
      "INFO:__main__:Successfully extracted 115 URLs from sitemap\n"
     ]
    }
   ],
   "source": [
    "urls_info = extract_urls_from_sitemap(\"https://ssl.robocup.org/page-sitemap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URLs: 115\n",
      "First 2 URLs: ['https://ssl.robocup.org/rules/', 'https://ssl.robocup.org/robocups/robocup-2019/robocup-2019-committees/']\n",
      "Last 2 URLs: ['https://ssl.robocup.org/technical-overview-of-the-small-size-league/', 'https://ssl.robocup.org/robocups/robocup-2025/robocup-2025-teams/']\n"
     ]
    }
   ],
   "source": [
    "website_urls = [info['url'] for info in urls_info]\n",
    "\n",
    "print(f\"Total URLs: {len(website_urls)}\")\n",
    "print(f\"First 2 URLs: {website_urls[:2]}\")\n",
    "print(f\"Last 2 URLs: {website_urls[-2:]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the text using tiktoken.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to count tokens for\n",
    "        model (str): The tokenizer model to use (default: cl100k_base for GPT-4)\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of tokens in the text\n",
    "    \"\"\"\n",
    "    encoder = tiktoken.get_encoding(model)\n",
    "    return len(encoder.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs4_extractor(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # Target the main article content for documentation \n",
    "    main_content = soup.find(\"article\", class_=\"md-content__inner\")\n",
    "    \n",
    "    # If found, use that, otherwise fall back to the whole document\n",
    "    content = main_content.get_text() if main_content else soup.text\n",
    "    \n",
    "    # Clean up whitespace\n",
    "    content = re.sub(r\"\\n\\n+\", \"\\n\\n\", content).strip()\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_site():\n",
    "    \"\"\"\n",
    "    Load information from the official website.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveUrlLoader to fetch pages from the website\n",
    "    2. Counts the total documents and tokens loaded\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of Document objects containing the loaded content\n",
    "        list: A list of tokens per document\n",
    "    \"\"\"\n",
    "    print(\"Loading website...\")\n",
    "\n",
    "    docs = []\n",
    "    for url in website_urls:\n",
    "\n",
    "        loader = RecursiveUrlLoader(\n",
    "            url,\n",
    "            max_depth=5,\n",
    "            extractor=bs4_extractor,\n",
    "        )\n",
    "\n",
    "        # Load documents using lazy loading (memory efficient)\n",
    "        docs_lazy = loader.lazy_load()\n",
    "\n",
    "        # Load documents and track URLs\n",
    "        for d in docs_lazy:\n",
    "            docs.append(d)\n",
    "\n",
    "    print(f\"Loaded {len(docs)} documents from website.\")\n",
    "    print(\"\\nLoaded URLs:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"{i+1}. {doc.metadata.get('source', 'Unknown URL')}\")\n",
    "    \n",
    "    # Count total tokens in documents\n",
    "    total_tokens = 0\n",
    "    tokens_per_doc = []\n",
    "    for doc in docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "        tokens_per_doc.append(count_tokens(doc.page_content))\n",
    "    print(f\"Total tokens in loaded documents: {total_tokens}\")\n",
    "    \n",
    "    return docs, tokens_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_full_website(documents):\n",
    "    \"\"\" Save the documents to a file \"\"\"\n",
    "\n",
    "    # Open the output file\n",
    "    output_filename = \"full_website.txt\"\n",
    "\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        # Write each document\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Get the source (URL) from metadata\n",
    "            source = doc.metadata.get('source', 'Unknown URL')\n",
    "            \n",
    "            # Write the document with proper formatting\n",
    "            f.write(f\"DOCUMENT {i+1}\\n\")\n",
    "            f.write(f\"SOURCE: {source}\\n\")\n",
    "            f.write(\"CONTENT:\\n\")\n",
    "            f.write(doc.page_content)\n",
    "            f.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
    "\n",
    "    print(f\"Documents concatenated into {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"\n",
    "    Split documents into smaller chunks for improved retrieval.\n",
    "    \n",
    "    This function:\n",
    "    1. Uses RecursiveCharacterTextSplitter with tiktoken to create semantically meaningful chunks\n",
    "    2. Ensures chunks are appropriately sized for embedding and retrieval\n",
    "    3. Counts the resulting chunks and their total tokens\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects to split\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of split Document objects\n",
    "    \"\"\"\n",
    "    print(\"Splitting documents...\")\n",
    "    \n",
    "    # Initialize text splitter using tiktoken for accurate token counting\n",
    "    # chunk_size=8,000 creates relatively large chunks for comprehensive context\n",
    "    # chunk_overlap=500 ensures continuity between chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=8000,  \n",
    "        chunk_overlap=500  \n",
    "    )\n",
    "    \n",
    "    # Split documents into chunks\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"Created {len(split_docs)} chunks from documents.\")\n",
    "    \n",
    "    # Count total tokens in split documents\n",
    "    total_tokens = 0\n",
    "    for doc in split_docs:\n",
    "        total_tokens += count_tokens(doc.page_content)\n",
    "    \n",
    "    print(f\"Total tokens in split documents: {total_tokens}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectorstore(splits):\n",
    "    \"\"\"\n",
    "    Create a vector store from document chunks using SKLearnVectorStore.\n",
    "    \n",
    "    This function:\n",
    "    1. Initializes an embedding model to convert text into vector representations\n",
    "    2. Creates a vector store from the document chunks\n",
    "    \n",
    "    Args:\n",
    "        splits (list): List of split Document objects to embed\n",
    "        \n",
    "    Returns:\n",
    "        SKLearnVectorStore: A vector store containing the embedded documents\n",
    "    \"\"\"\n",
    "    print(\"Creating SKLearnVectorStore...\")\n",
    "    \n",
    "    # Initialize OpenAI embeddings\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    \n",
    "    # Create vector store from documents using SKLearn\n",
    "    persist_path = os.getcwd()+\"/sklearn_vectorstore.parquet\"\n",
    "    vectorstore = SKLearnVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_path=persist_path   ,\n",
    "        serializer=\"parquet\",\n",
    "    )\n",
    "    print(\"SKLearnVectorStore created successfully.\")\n",
    "    \n",
    "    vectorstore.persist()\n",
    "    print(\"SKLearnVectorStore was persisted to\", persist_path)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métodos definidos.\n",
    "\n",
    "Agora vamos de fato fazer o Scrapping do site da Capyba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Capyba website...\n",
      "Loaded 16 documents from Capyba documentation.\n",
      "\n",
      "Loaded URLs:\n",
      "1. https://www.capyba.com/\n",
      "2. https://www.capyba.com/cases/case-genomika-en\n",
      "3. https://www.capyba.com/cases/cases-home\n",
      "4. https://www.capyba.com/cases/case-inadash-en\n",
      "5. https://www.capyba.com/about-us\n",
      "6. https://www.capyba.com/our-process\n",
      "7. https://www.capyba.com/blog\n",
      "8. https://www.capyba.com/careers\n",
      "9. https://www.capyba.com/get-in-touch\n",
      "10. https://www.capyba.com/services\n",
      "11. https://www.capyba.com/cases/case-fretapp-en\n",
      "12. https://www.capyba.com/capybaday\n",
      "13. https://www.capyba.com/services\n",
      "14. https://www.capyba.com/our-process\n",
      "15. https://www.capyba.com/cases/cases-home\n",
      "16. https://www.capyba.com/about-us\n",
      "Total tokens in loaded documents: 12385\n"
     ]
    }
   ],
   "source": [
    "# Load the documents\n",
    "documents, tokens_per_doc = load_site()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the amount of documents\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirming the amount of tokens report\n",
    "len(tokens_per_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First document has 536 tokens. Content:\n",
      "page_content='Capyba Software - Driving Transformation\n",
      "\n",
      "ServicesOur processCasesBlogAboutGet in TouchJoin the Magic Team 🧙MenuMenuServicesOur ProcessCasesBlogAboutJoin the Magic Team 🧙Get in touchReady for#CapybaDay2023?Learn moreWe transformideas into digital products  Discover how we can help youGet in touchWhat We DoWe create, co-create and transform the digital business of global enterprises and help them grow.SoftwareDevelopmentSolid experience in building and scaling software on mobile, web and connected plataforms.See moreSee moreProductDesignDesigning delightful experiences with a human-centered approach by involving human perspective.See moreSee moreTechConsultingDigital transformation is not just a buzzword, it's the key to maintain your business at a high level.See moreSee moreCasesAlbert Einstein: GenomikaA web system to generate automatic medical reports from parameters and configurable rules based on genetic tests.TECHNOLOGIES:TOOLS:See moreFretAppUX design and development of the whole platform. Including a mobile app and a web system. The marketplace can be compared to \"uber for truck drivers\".TECHNOLOGIES:See moreInadashWe developed an app that provides a new approach between landlords and tenants in the home rental in England.TECHNOLOGIES:See moreView more casesSee what our clientssay about usThe team always showed themselve to be very concerned with the growth of my businessAlyson Tabosa, CoteAquiSee full feedbackWe Deliver!Count on us to achieve your desired structure, in the right time. It’s our DNA to pave the way for your business to get there!CAPYBALOGLogistics TrackingCapypaLog provides, in real time, the status of receipt, stock, conference, shipping, transportation and delivery of your products with transparency throughout the process.See moreSee moreDigital and analog content.Driving transformation.Sara MárquezJanuary 09, 2023Step By Step Guide To Design Process: The Capyba Way.Step By Step Guide To Design Process: The Capyba WaySara MárquezDecember 23, 2022Top App Development Trends Business Should Look Up In 2023View more postsAre you leading the change or are you being led by it?Let's work together!What are you waiting for?Share your ideas and goals over a coffee with us!Remotely or not! ;)Capyba Software, 2021 - Made with 💚 in Recife, Brazil. Directly from the Manguezal.By browsing this site, you agree to our Cookies Policy' metadata={'source': 'https://www.capyba.com/', 'content_type': 'text/html', 'title': 'Capyba Software - Driving Transformation', 'description': 'We transform ideas into digital products and boost your development team. We are a design-driven software studio. We work with technology to create, co-create and transform the digital business.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "# Seeing a preview of a document with the number of tokens\n",
    "print(f\"First document has {tokens_per_doc[0]} tokens. Content:\\n{documents[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents concatenated into capyba_full.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the documents to a file\n",
    "save_full_website(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n",
      "Created 16 chunks from documents.\n",
      "Total tokens in split documents: 12385\n"
     ]
    }
   ],
   "source": [
    "# Split the documents\n",
    "split_docs = split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SKLearnVectorStore...\n",
      "SKLearnVectorStore created successfully.\n",
      "SKLearnVectorStore was persisted to /home/tone/www/capyba/capyba_mcp/sklearn_vectorstore.parquet\n"
     ]
    }
   ],
   "source": [
    "# Create the vector store\n",
    "vectorstore = create_vectorstore(split_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronto. Com esses passos criamos um vectorstore que salvou todos documentos extraidos do site da Capyba.\n",
    "\n",
    "Vamos fazer alguns testes para garantir que funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever to get relevant documents (k=3 means return top 3 matches)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Who are the people that Work at Capyba ?\n",
      "Retrieved 3 relevant documents\n",
      "https://www.capyba.com/about-us\n",
      "About Us\n",
      "ServicesOur processCasesBlogAboutGet in TouchJoin the Magic Team 🧙MenuMenuServicesOur ProcessCasesBlogAboutJoin the Magic Team 🧙Get in touchAbout UsWe're Capyba.A software studiomade by peopleWe seek innovation with personal growth, a fair and balanced work environment, transforming and generating social impact.This is being Capyba.Alessa AlvesProject Manageralessa@capyba.comAmanda CamposOperations Analystalessa@capyba.comAntônio GabrielSoftware Engineeralessa@capyba.comArmanda MariaPro\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://www.capyba.com/about-us\n",
      "About Us\n",
      "ServicesOur processCasesBlogAboutGet in TouchJoin the Magic Team 🧙MenuMenuServicesOur ProcessCasesBlogAboutJoin the Magic Team 🧙Get in touchAbout UsWe're Capyba.A software studiomade by peopleWe seek innovation with personal growth, a fair and balanced work environment, transforming and generating social impact.This is being Capyba.Alessa AlvesProject Manageralessa@capyba.comAmanda CamposOperations Analystalessa@capyba.comAntônio GabrielSoftware Engineeralessa@capyba.comArmanda MariaPro\n",
      "\n",
      "--------------------------------\n",
      "\n",
      "https://www.capyba.com/about-us\n",
      "About Us\n",
      "ServicesOur processCasesBlogAboutGet in TouchJoin the Magic Team 🧙MenuMenuServicesOur ProcessCasesBlogAboutJoin the Magic Team 🧙Get in touchAbout UsWe're Capyba.A software studiomade by peopleWe seek innovation with personal growth, a fair and balanced work environment, transforming and generating social impact.This is being Capyba.Alessa AlvesProject Manageralessa@capyba.comAmanda CamposOperations Analystalessa@capyba.comAntônio GabrielSoftware Engineeralessa@capyba.comArmanda MariaPro\n",
      "\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get relevant documents for the query\n",
    "query = \"What is the size of the goal ?\"    \n",
    "relevant_docs = retriever.invoke(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
    "\n",
    "for d in relevant_docs:\n",
    "    print(d.metadata['source'])\n",
    "    print(d.page_content[0:500])\n",
    "    print(\"\\n--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você abrir o site, provavelmente vai entrar a página com os membros do time da Capyba.\n",
    "\n",
    "> Inclusive, não estou lá :/\n",
    "\n",
    "Com essa informação, uma LLM poderá gerar uma resposta!\n",
    "\n",
    "> Daria para melhorar muito a extração de texto, removendo o cabeçalho e o footer, por exemplo. Aceitamos improovements!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora bora botar as LLM pra trabalhar!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
